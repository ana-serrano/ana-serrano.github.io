<!DOCTYPE html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Movie Editing and Cognitive Event Segmentation in Virtual Reality Video</title>
<!--[if IE]>
<link href="../shared/style-ie.css" rel="stylesheet" type="text/css">
<![endif]-->
<link href="./assets/style.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="./assets/scroll.js"></script>
</head>

<body>
<a id="top"></a>
<div id="wrapper">
<div id="header">
<div id="journal">ACM Transactions on Graphics Vol.36(4), SIGGRAPH 2017</div>
<div id="title"><a href="https://ana-serrano.github.io/projects/VR-cinematography.html" class="nounderline">Movie Editing and Cognitive Event Segmentation in Virtual Reality Video</a></div>


<table id="authors">
<tbody><tr>
<td><a href="http://webdiis.unizar.es/~aserrano/">Ana Serrano</a><span class="super">1</span></td>
<td><a>Vincent Sitzmann</a><span class="super">2</span></td>
<td><a>Jaime Ruiz-Borau</a><span class="super">1</span></td>
<td><a href="https://web.stanford.edu/~gordonwz/">Gordon Wetzstein</a><span class="super">2</span></td>
<td><a href="http://giga.cps.unizar.es/~diegog/">Diego Gutierrez</a><span class="super">1</span></td>
<td><a href="http://webdiis.unizar.es/~bmasia/">Belen Masia</a><span class="super">1</span></td>
</tr>
<!--<tr class="mail">
<td><img src="../shared/mails/bmasia_rr.png"></td>
</tr>-->

<tr>
<td colspan="7" id="affiliation">
<span class="super">1</span> Universidad de Zaragoza, I3A
<span class="super">2</span> Stanford University
</td></tr>
</tbody></table>


<a href="https://ana-serrano.github.io/projects/images/teaser_VR-cine.jpg" class="nounderline"><img style="width:850px;" src="./images/teaser_VR-cine.jpg" alt="Teaser" id="teaser"></a>


<table id="navigation">
<tbody><tr>
<td><a href="https://ana-serrano.github.io/projects/VR-cinematography.html#news">News</a></td>
<td><a href="https://ana-serrano.github.io/projects/VR-cinematography.html#abstract">Abstract</a></td>
<td><a href="https://ana-serrano.github.io/projects/VR-cinematography.html#downloads">Downloads</a></td>
<td><a href="https://ana-serrano.github.io/projects/VR-cinematography.html#bibtex">Bibtex</a></td>
<!--<td><a href="https://ana-serrano.github.io/projects/VR-cinematography.html#links">Links</a></td>-->
<td><a href="https://ana-serrano.github.io/projects/VR-cinematography.html#related">Related</a></td>
</tr>
</tbody></table>
</div>


<div id="content">

<h1><a id="news" href="https://ana-serrano.github.io/projects/VR-cinematography.html#top">News</a></h1>
<ul>
<li><span class="italic">September, 2017</span>: Full video/audio dataset and eyetracking data available (<span class="italic">see <a href="https://ana-serrano.github.io/projects/VR-cinematography.html#downloads">Downloads</a></span>).</li>
<li><span class="italic">April, 2017</span>:  Web launched.</li>
<li><span class="italic">May, 2016</span>: Paper available (<span class="italic">see <a href="https://ana-serrano.github.io/projects/VR-cinematography.html#downloads">Downloads</a></span>).</li>
</ul>


<h1><a id="abstract" href="https://ana-serrano.github.io/projects/VR-cinematography.html#top">Abstract</a></h1>
<p>
Traditional cinematography has relied for over a century on a well-established
set of editing rules, called continuity editing, to create a sense of situational
continuity. Despite massive changes in visual content across cuts, viewers
in general experience no trouble perceiving the discontinuous flow of information
as a coherent set of events. However, Virtual Reality (VR) movies
are intrinsically different from traditional movies in that the viewer controls
the camera orientation at all times. As a consequence, common editing techniques
that rely on camera orientations, zooms, etc., cannot be used. In this
paper we investigate key relevant questions to understand how well traditional
movie editing carries over to VR, such as: Does the perception of
continuity hold across edit boundaries? Under which conditions? Do viewers’
observational behavior change after the cuts? To do so, we rely on recent
cognition studies and the event segmentation theory, which states that our
brains segment continuous actions into a series of discrete, meaningful events.
We first replicate one of these studies to assess whether the predictions of
such theory can be applied to VR. On a next stage, we gather gaze data
from viewers watching VR videos containing different edits with varying
parameters, and provide the first systematic analysis of viewers’ behavior and
the perception of continuity in VR. From this analysis we make a series of
relevant findings; for instance, our data suggests that predictions from the
cognitive event segmentation theory are useful guides for VR editing; that
different types of edits are equally well understood in terms of continuity; and
that spatial misalignments between regions of interest at the edit boundaries
favor a more exploratory behavior even after viewers have fixated on a new
region of interest. In addition, we propose a number of metrics to describe
viewers attentional behavior in VR. We believe the insights derived from our
work can be useful as guidelines for VR content creation.

<h1><a id="downloads" href="https://ana-serrano.github.io/projects/VR-cinematography.html#top">Downloads</a></h1>
<ul>
<li><a target="_blank" href="https://www.dropbox.com/s/xbwjs19jve2zuxy/Serrano_SIGG2017_VR-cine.pdf?raw=1">Paper [PDF, 24.8 MB]</a></li>
<li><a target="_blank" href="https://www.dropbox.com/s/2mxg7w3sktyrvru/Serrano_SIGG2017_VR-cine_supp.zip?raw=1">Supplementary [ZIP, 132.48 MB]</a></li>
<li><a target="_blank" href="https://www.dropbox.com/s/7umz2lqakhpwt23/Serrano_SIGG2017_VR-cine.pptx?raw=1">Slides [PPTX, 301 MB]</a></li>
<li><a target="_blank" href="http://webdiis.unizar.es/~aserrano/docs/AnaSerrano_SIGG2017_VR-cine_code.zip">Code and data [ZIP, 266.44 MB]</a></li>
<li><a target="_blank" href="https://drive.google.com/drive/folders/0B6OEXFkAawbNdUxTRlZid0tNTk0?usp=sharing">Full video/audio dataset and eyetracking data [shared folder]</a></li>
</ul>
<p><i>The code and dataset provided are property of Universidad de Zaragoza - free for non-commercial purposes</i></p>

<h1><a id="bibtex" href="https://ana-serrano.github.io/projects/VR-cinematography.html#top">Bibtex</a></h1>
<div id="bibtexsec">
@article{Serrano_VR-cine_SIGGRAPH2017, 
author = {Serrano, Ana and Sitzmann, Vincent and Ruiz-Borau, Jaime and Wetzstein, Gordon and Gutierrez, Diego and Masia, Belen}, 
title = {Movie Editing and Cognitive Event Segmentation in Virtual Reality Video}, 
journal = {ACM Transactions on Graphics (SIGGRAPH 2017)}, 
volume = {36}, 
number = {4}, 
year = {2017}
}

</div>



<h1><a id="related" href="#top">Related</a></h1>
<ul>
<li>2017: <a target="_blank" href="https://vsitzmann.github.io/vr-saliency/">Saliency in VR: How do people explore virtual environments?</a></li>
</ul>

<h1><a id="acks" href="#top">Acknowledgements</a></h1>
<p>
We would like to thank Paz Hernando and Marta Ortin for their
help with the experiments and analyses. We would also like to
thank CubeFX for allowing us to reproduce Star Wars - Hunting of
the Fallen, and Abaco digital for helping us record the videos for
creating the stimuli, as well as Sandra Malpica and Victor Arellano
for being great actors. This research has been partially funded by an
ERC Consolidator Grant (project CHAMELEON), and the Spanish
Ministry of Economy and Competitiveness (projects TIN2016-78753-
P, TIN2016-79710-P, and TIN2014-61696-EXP). Ana Serrano was
supported by an FPI grant from the Spanish Ministry of Economy
and Competitiveness. Diego Gutierrez was additionally funded by a
Google Faculty Research Award and the BBVA Foundation. Gordon
Wetzstein was supported by a Terman Faculty Fellowship, an Okawa
Research Grant, and an NSF Faculty Early Career Development
(CAREER) Award.

</p>


</div>
</div>


</body></html>